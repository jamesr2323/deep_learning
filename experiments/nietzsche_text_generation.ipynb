{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All imports needed\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "from fastai.imports import *\n",
    "\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating new Nietsche text\n",
    "\n",
    "This is primarily just a notebook to implement myself Lesson 6 of the FastAI Part 1 Deep Learning course. https://github.com/fastai/fastai/blob/master/courses/dl1/lesson6-rnn.ipynb\n",
    "\n",
    "First up we need to get hold of the data - the entire collected works of Nietzsche:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "PATH = '../data/nietzsche/'\n",
    "\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "urllib.request.urlretrieve(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\n",
    "\n",
    "text = open(f'{PATH}nietzsche.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n",
      "total chars: 85 : ['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Æ', 'ä', 'æ', 'é', 'ë']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "\n",
    "print('corpus length:', len(text))\n",
    "print('total chars:', vocab_size, ':', chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than use characters for the model - we will use numbers. Ie \"HELLO\" could become `[8,5,12,12,15]`. So we map every charcater to an index 0..84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '=': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, '[': 50, ']': 51, '_': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, 'Æ': 79, 'ä': 80, 'æ': 81, 'é': 82, 'ë': 83}\n"
     ]
    }
   ],
   "source": [
    "chars_to_indexes = {i: x for x,i in enumerate(chars)}\n",
    "indexes_to_chars = {x: i for x,i in enumerate(chars)}\n",
    "\n",
    "print(chars_to_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, convert the entire corpus to indexes to use as the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 67, 1, 54, 57, 1, 75, 67, 66, 21, 1, 53, 66, 56, 0, 53, 72, 1, 68, 70, 57, 71, 57, 66, 72, 1, 57, 74, 57, 70, 77, 1, 63, 61, 66, 56, 1, 67, 58, 1, 56, 67, 59, 65, 53, 1, 71, 72, 53, 66, 56, 71, 1, 75, 61, 72, 60, 1, 71, 53, 56, 1, 53, 66, 56, 1, 56, 61, 71, 55, 67, 73, 70, 53, 59, 57, 56, 1, 65, 61, 57, 66, 8, 8, 32, 29, 7, 0, 61, 66, 56, 57, 57, 56, 7, 1, 61, 72, 1, 71]\n",
      "to be won; and\n",
      "at present every kind of dogma stands with sad and discouraged mien--IF,\n",
      "indeed, it s\n"
     ]
    }
   ],
   "source": [
    "idxs = [chars_to_indexes[char] for char in text]\n",
    "\n",
    "print(idxs[400:500])\n",
    "print(''.join([indexes_to_chars[idx] for idx in idxs[400:500]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-character model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're going to start off with looking at just the last 3 characters and trying to predict the 4th.\n",
    "\n",
    "To begin, create a 4 arrays, each one offset one from the last (3 for the inputs, and one for y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 41, 28, 29, 24, 26, 28, 0, 0, 0, 42, 44, 39, 39, 38, 42, 32, 37, 30, 1]\n",
      "[39, 29, 28, 0, 39, 42, 30, 60, 1, 73, 1, 1, 75, 53, 8, 53, 72, 66, 32, 72]\n",
      "[41, 24, 0, 42, 39, 32, 1, 53, 43, 72, 61, 53, 67, 66, 75, 72, 60, 23, 71, 60]\n",
      "[28, 26, 0, 44, 38, 37, 72, 72, 70, 60, 71, 1, 65, 8, 60, 1, 57, 1, 1, 57]\n",
      "[29, 28, 0, 39, 42, 30, 60, 1, 73, 1, 1, 75, 53, 8, 53, 72, 66, 32, 72, 70]\n"
     ]
    }
   ],
   "source": [
    "cs = 3\n",
    "\n",
    "c1_dat = [idxs[i] for i in range(0, len(idxs)-cs, cs)]\n",
    "c2_dat = [idxs[i+1] for i in range(0, len(idxs)-cs, cs)]\n",
    "c3_dat = [idxs[i+2] for i in range(0, len(idxs)-cs, cs)]\n",
    "c4_dat = [idxs[i+3] for i in range(0, len(idxs)-cs, cs)]\n",
    "\n",
    "print(idxs[:20])\n",
    "print(c1_dat[:20])\n",
    "print(c2_dat[:20])\n",
    "print(c3_dat[:20])\n",
    "print(c4_dat[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the above that this is like taking every four characters and putting them in columns. The first three characters will be the inputs to the neural net and the last is the output we're optimising for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.stack(c1_dat)\n",
    "x2 = np.stack(c2_dat)\n",
    "x3 = np.stack(c3_dat)\n",
    "\n",
    "y = np.stack(c4_dat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
