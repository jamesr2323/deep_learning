{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All imports needed\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from fastai.io import *\n",
    "from fastai.conv_learner import *\n",
    "\n",
    "from fastai.column_data import *\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating new Nietsche text\n",
    "\n",
    "This is primarily just a notebook to implement myself Lesson 6 of the FastAI Part 1 Deep Learning course. https://github.com/fastai/fastai/blob/master/courses/dl1/lesson6-rnn.ipynb\n",
    "\n",
    "First up we need to get hold of the data - the entire collected works of Nietzsche:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data/nietzsche/'\n",
    "\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "urllib.request.urlretrieve(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\n",
    "\n",
    "text = open(f'{PATH}nietzsche.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n",
      "total chars: 85 : ['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Æ', 'ä', 'æ', 'é', 'ë']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "\n",
    "print('corpus length:', len(text))\n",
    "print('total chars:', vocab_size, ':', chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than use characters for the model - we will use numbers. Ie \"HELLO\" could become `[8,5,12,12,15]`. So we map every charcater to an index 0..84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '=': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, '[': 50, ']': 51, '_': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, 'Æ': 79, 'ä': 80, 'æ': 81, 'é': 82, 'ë': 83}\n"
     ]
    }
   ],
   "source": [
    "chars_to_indexes = {i: x for x,i in enumerate(chars)}\n",
    "indexes_to_chars = {x: i for x,i in enumerate(chars)}\n",
    "\n",
    "print(chars_to_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, convert the entire corpus to indexes to use as the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 67, 1, 54, 57, 1, 75, 67, 66, 21, 1, 53, 66, 56, 0, 53, 72, 1, 68, 70, 57, 71, 57, 66, 72, 1, 57, 74, 57, 70, 77, 1, 63, 61, 66, 56, 1, 67, 58, 1, 56, 67, 59, 65, 53, 1, 71, 72, 53, 66, 56, 71, 1, 75, 61, 72, 60, 1, 71, 53, 56, 1, 53, 66, 56, 1, 56, 61, 71, 55, 67, 73, 70, 53, 59, 57, 56, 1, 65, 61, 57, 66, 8, 8, 32, 29, 7, 0, 61, 66, 56, 57, 57, 56, 7, 1, 61, 72, 1, 71]\n",
      "to be won; and\n",
      "at present every kind of dogma stands with sad and discouraged mien--IF,\n",
      "indeed, it s\n"
     ]
    }
   ],
   "source": [
    "idxs = [chars_to_indexes[char] for char in text]\n",
    "\n",
    "print(idxs[400:500])\n",
    "print(''.join([indexes_to_chars[idx] for idx in idxs[400:500]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-character model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're going to start off with looking at just the last 3 characters and trying to predict the 4th.\n",
    "\n",
    "To begin, create a 4 arrays, each one offset one from the last (3 for the inputs, and one for y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 41, 28, 29, 24, 26, 28, 0, 0, 0, 42, 44, 39, 39, 38, 42, 32, 37, 30, 1]\n",
      "[39, 29, 28, 0, 39, 42, 30, 60, 1, 73, 1, 1, 75, 53, 8, 53, 72, 66, 32, 72]\n",
      "[41, 24, 0, 42, 39, 32, 1, 53, 43, 72, 61, 53, 67, 66, 75, 72, 60, 23, 71, 60]\n",
      "[28, 26, 0, 44, 38, 37, 72, 72, 70, 60, 71, 1, 65, 8, 60, 1, 57, 1, 1, 57]\n",
      "[29, 28, 0, 39, 42, 30, 60, 1, 73, 1, 1, 75, 53, 8, 53, 72, 66, 32, 72, 70]\n"
     ]
    }
   ],
   "source": [
    "cs = 3\n",
    "\n",
    "c1_dat = [idxs[i] for i in range(0, len(idxs)-cs, cs)]\n",
    "c2_dat = [idxs[i+1] for i in range(0, len(idxs)-cs, cs)]\n",
    "c3_dat = [idxs[i+2] for i in range(0, len(idxs)-cs, cs)]\n",
    "c4_dat = [idxs[i+3] for i in range(0, len(idxs)-cs, cs)]\n",
    "\n",
    "print(idxs[:20])\n",
    "print(c1_dat[:20])\n",
    "print(c2_dat[:20])\n",
    "print(c3_dat[:20])\n",
    "print(c4_dat[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the above that this is like taking every four characters and putting them in columns. The first three characters will be the inputs to the neural net and the last is the output we're optimising for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.stack(c1_dat)\n",
    "x2 = np.stack(c2_dat)\n",
    "x3 = np.stack(c3_dat)\n",
    "\n",
    "y = np.stack(c4_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the PyTorch model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Char3Model(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The four layers of our model\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.l_in = nn.Linear(n_fac, n_hidden)\n",
    "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, c1, c2, c3):\n",
    "        in1 = F.relu(self.l_in(self.e(c1)))\n",
    "        in2 = F.relu(self.l_in(self.e(c2)))\n",
    "        in3 = F.relu(self.l_in(self.e(c3)))\n",
    "        \n",
    "        h = V(torch.zeros(in1.size()))\n",
    "        h = F.tanh(self.l_hidden(h+in1))\n",
    "        h = F.tanh(self.l_hidden(h+in2))\n",
    "        h = F.tanh(self.l_hidden(h+in3))\n",
    "        \n",
    "        return F.log_softmax(self.l_out(h))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays('.', [-1], np.stack([x1,x2,x3], axis=1), y, bs=512)\n",
    "\n",
    "vocab_size = len(chars)+1\n",
    "n_fac = 42 # embedding matrix width\n",
    "n_hidden = 256 # number hidden units\n",
    "\n",
    "m = Char3Model(vocab_size, n_fac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up iterator to load data in batches\n",
    "it = iter(md.trn_dl)\n",
    "*xs,ys = next(it)\n",
    "t = m(*V(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea2e4c7d4084e059a7f4e90ea853192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      2.064185   0.504944  \n",
      "    1      2.025913   0.30025                               \n",
      "    2      1.990613   0.700232                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.70023])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the optimizer\n",
    "opt = optim.Adam(m.parameters(), 1e-2)\n",
    "fit(m, md, 3, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27ca47e881f48e1aeffa6d6e12cc98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.758938   0.473853  \n",
      "    1      1.720331   0.271136                              \n",
      "    2      1.717027   0.34821                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.34821])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(opt, 1e-3)\n",
    "fit(m, md, 3, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out the model\n",
    "\n",
    "Ok so we trained a model... made some graphs appear etc. But how do we use it?\n",
    "\n",
    "`m(1,2,3)` - throws an error, because pyTorch wants each argument to be turned into pyTorch variable. \n",
    "\n",
    "`m(V(1),V(2),V(3))` gives us a `[torch.cuda.FloatTensor of size 1x85 (GPU 0)]` - getting better\n",
    "\n",
    "Let's try and see that as a familiar numpy ndarray:\n",
    "\n",
    "`m(V(1),V(2),V(3)).data.numpy()` - another error, this time because the Tensor is on the GPU memory, and needs to be moved into system RAM before we can use it\n",
    "\n",
    "`m(V(1),V(2),V(3)).cpu().data.numpy()` - tada!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.29849,  -2.36959,  -5.86239,  -5.60455, -11.29167, -10.35843,  -7.01992,  -6.12142,  -1.83529,\n",
       "         -5.39128,  -8.92203,  -9.23298,  -6.56709,  -9.06264,  -8.02441,  -9.74576, -11.53138,  -7.5312 ,\n",
       "         -7.07608,  -7.60975,  -8.25402,  -6.13394,  -9.88077,  -7.8184 ,  -4.78491,  -5.76463,  -6.19066,\n",
       "         -6.91619,  -5.15337,  -3.95679,  -6.2584 ,  -4.86974,  -2.53861,  -5.28917,  -4.32854,  -3.53656,\n",
       "         -1.6944 ,  -4.60295,  -4.51753,  -3.49602,  -6.47315,  -7.91241,  -3.81931,  -1.70739,  -7.24739,\n",
       "         -5.70486,  -4.0613 ,  -9.3907 ,  -6.80647, -10.23992,  -6.1278 , -12.37789, -10.94001,  -6.6625 ,\n",
       "         -7.03732,  -5.12533,  -6.60228,  -8.08359,  -7.62465,  -5.59865,  -7.42388,  -7.48192,  -7.37621,\n",
       "         -9.43708,  -8.20658,  -4.77836,  -5.8834 ,  -8.34979,  -6.83678,  -7.02198,  -9.05406,  -7.67219,\n",
       "         -4.35888,  -9.24077,  -8.3455 ,  -5.21575, -12.29481, -12.27484, -14.51767, -13.27466, -15.02666,\n",
       "        -13.4426 , -16.30942, -13.96821, -13.32289]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(V(1),V(2),V(3)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the above are log likelihoods that the next letter is in one of our 85 vocab letters. Let's wrap this into a little function to make it easier to work with using a couple of FastsAI helper functions to make our lives a little smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the next n letters given an input block\n",
    "def get_next(inp):\n",
    "    idxs = T(np.array([chars_to_indexes[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]\n",
    "\n",
    "def get_next_n(inp, n):\n",
    "    for i in range(0, n):\n",
    "        inp += get_next(inp[-3:])\n",
    "    \n",
    "    return inp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Children the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some and the some a'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n('Children ', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the most compelling Nietzsche copy I have to admit. But it's a start!\n",
    "\n",
    "# Recurrent Neural Network\n",
    "\n",
    "Start by creating the data, this time using a rolling window of 8 characters across the entire sequence. We'll also create our `y`, with the next character after each of the windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39 41 28 ..., 26 28  0]\n",
      " [41 28 29 ..., 28  0  0]\n",
      " [28 29 24 ...,  0  0  0]\n",
      " ..., \n",
      " [71 61 66 ..., 64 66 57]\n",
      " [61 66 58 ..., 66 57 71]\n",
      " [66 58 73 ..., 57 71 71]] [ 0  0 42 ..., 71 71  9]\n"
     ]
    }
   ],
   "source": [
    "input_length = 8\n",
    "\n",
    "inputs = [[idxs[i+j] for i in range(input_length)] for j in range(len(idxs) - input_length)]\n",
    "y = [idxs[j+input_length] for j in range(len(idxs) - input_length)]\n",
    "\n",
    "X = np.stack(inputs, axis = 0)\n",
    "y = np.stack(y)\n",
    "\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our modified RNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLoopModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.l_in = nn.Linear(n_fac, n_hidden)\n",
    "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "    \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(bs, n_hidden).cuda())\n",
    "        for c in cs:\n",
    "            inp = F.relu(self.l_in(self.e(c)))\n",
    "            h = F.tanh(self.l_hidden(h + inp))\n",
    "            \n",
    "        return F.log_softmax(self.l_out(h), dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take it for a spin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e04c64c71a144459d756b9ac8eaf423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      2.107797   0.843655  \n",
      "    1      2.04132    1.395311                              \n",
      "    2      2.007522   0.313711                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.31371])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = ColumnarModelData.from_arrays('.', [-1], X, y, bs=512)\n",
    "m = CharLoopModel(vocab_size, n_fac=42).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-2)\n",
    "fit(m, md, 3, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bla blation the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the his the'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n(\"bla bla\", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not so great... try more training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc07155f7fd4d35ad89ab26af2bbf1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.641391   0.287052  \n",
      "    1      1.643354   0.291294                              \n",
      "    2      1.631389   0.443712                              \n",
      "    3      1.638758   0.409489                              \n",
      "    4      1.622094   0.360929                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.36093])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(opt, 1e-3)\n",
    "fit(m, md, 5, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bla blace of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the spirity of the '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n(\"bla bla\", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well at least it's repeating five words now. That's a new record!\n",
    "\n",
    "Now we're going to try concatenating the activations of the next characters to the previous activations, instead of adding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLoopConcatModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.l_in = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(bs, n_hidden).cuda())\n",
    "        \n",
    "        for c in cs:\n",
    "            inp = torch.cat((h, self.e(c)), 1)\n",
    "            inp = F.relu(self.l_in(inp))\n",
    "            h = F.tanh(self.l_hidden(inp))\n",
    "            \n",
    "        return F.log_softmax(self.l_out(h), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = 42\n",
    "n_hidden = 256\n",
    "\n",
    "md = ColumnarModelData.from_arrays('.', [-1], X, y, bs=512)\n",
    "m = CharLoopConcatModel(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)\n",
    "\n",
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)\n",
    "t = m(*V(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3284252ae03e4d1ca637c2afb18b7dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.563395   3.046283  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2180505f2d43a39608de8fb0876297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.460188   2.628501  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 2.6285])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)\n",
    "set_lrs(opt, 1e-4)\n",
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to and to a'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n(\"The \", 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole sequence RNN\n",
    "\n",
    "Now we move onto predicting the sequence from the one which is offset one to the left of it. Ie `[n..n+sl-1]` from `[n-1..n+sl-2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WholeSequenceRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, n_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(1, bs, n_hidden))\n",
    "        \n",
    "        inp = self.e(torch.stack(cs))        \n",
    "        outp, h = self.rnn(inp, h)\n",
    "        \n",
    "        return F.log_softmax(self.l_out(outp), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75111, 8)\n",
      "(75111, 8)\n"
     ]
    }
   ],
   "source": [
    "sl = 8 # sequence_length\n",
    "\n",
    "# Split our array into sequences of length sl\n",
    "in_data = [[idxs[j+i] for i in range(0,sl)] for j in range(0, len(idxs) - sl - 1, sl)]\n",
    "\n",
    "# The y values are just the same sequences shifted along one value\n",
    "out_data = [[idxs[j+i+1] for i in range(0,sl)] for j in range(0, len(idxs) - sl - 1, sl)]\n",
    "\n",
    "X = np.stack(in_data)\n",
    "Y = np.stack(out_data)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977522a4ad7640b3ad4cba04351a2701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      2.596586   2.410337  \n",
      "    1      2.29198    2.199189                              \n",
      "    2      2.136583   2.082756                              \n",
      "    3      2.041392   2.010933                              \n",
      "    4      1.978309   1.959068                              \n",
      "    5      1.933227   1.921501                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 1.9215])]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Custom loss function for sequence to sequence\n",
    "def nll_loss_seq(inp, targ):\n",
    "    sl,bs,nh = inp.size()\n",
    "    targ = targ.transpose(0,1).contiguous().view(-1)\n",
    "    return F.nll_loss(inp.view(-1,nh), targ)\n",
    "    \n",
    "## Train the model\n",
    "val_idxs = get_cv_idxs(len(X)-sl-1)\n",
    "md = ColumnarModelData.from_arrays('.', val_idxs, X, Y, bs=512)\n",
    "\n",
    "vocab_size = len(chars) + 1\n",
    "n_fac = 42\n",
    "n_hidden = 256\n",
    "m = WholeSequenceRNN(vocab_size, n_fac, n_hidden).cuda()\n",
    "\n",
    "opt = optim.Adam(m.parameters(), 1e-3)\n",
    "\n",
    "it = iter(md.trn_dl)\n",
    "*xst,yt = next(it)\n",
    "\n",
    "fit(m, md, 6, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649efb70155f4e0fab88d86ce9280d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                             \n",
      "    0      1.895579   1.907868  \n",
      "    1      1.889812   1.903903                              \n",
      "    2      1.886362   1.900541                              \n",
      "    3      1.880677   1.897424                              \n",
      "    4      1.876248   1.894234                              \n",
      "    5      1.873578   1.891043                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 1.89104])]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(opt, 1e-4)\n",
    "fit(m, md, 6, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is outputting sequences instead of a single character, we must amend our `get_next_n` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 7 \n",
      "   -4.0743  -1.7731  -6.9447  -6.8811  -8.8905 -10.7962  -8.1841  -4.1221\n",
      "\n",
      "Columns 8 to 15 \n",
      "   -6.6479  -5.6189 -11.4422 -12.7250 -12.1732 -11.0300 -11.3725 -10.6553\n",
      "\n",
      "Columns 16 to 23 \n",
      "  -11.4044 -11.2764 -11.5660 -11.0987  -6.4641  -7.0537  -8.8155  -7.1522\n",
      "\n",
      "Columns 24 to 31 \n",
      "   -9.4181 -12.8529 -12.0324 -11.0236  -9.1678 -12.0218 -11.7190  -9.8195\n",
      "\n",
      "Columns 32 to 39 \n",
      "   -9.5945 -11.7516 -12.1323 -10.0555 -11.5064 -12.4759  -9.2542 -12.3241\n",
      "\n",
      "Columns 40 to 47 \n",
      "  -12.1797 -10.6150 -10.6228 -12.3198 -11.7007 -12.7769 -12.8490 -11.3206\n",
      "\n",
      "Columns 48 to 55 \n",
      "  -10.6881 -10.9370  -9.5313  -9.2135  -8.8469  -3.2702  -9.8773  -8.0981\n",
      "\n",
      "Columns 56 to 63 \n",
      "  -11.1514  -2.4108  -7.5576 -10.1757  -1.0893  -2.2201 -12.7098 -10.5652\n",
      "\n",
      "Columns 64 to 71 \n",
      "   -5.0654  -8.1782  -8.0295  -2.5045 -11.1886 -11.1160  -3.1864  -3.6936\n",
      "\n",
      "Columns 72 to 79 \n",
      "   -5.0507  -3.7543  -9.7235  -6.5563 -12.0107  -3.6839  -9.7306 -11.7255\n",
      "\n",
      "Columns 80 to 84 \n",
      "  -11.2387 -11.7565 -11.5410 -11.1430 -11.4176\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 7 \n",
      "   -4.5561  -2.3844  -6.5648  -6.5579  -7.5822 -11.8478  -7.5928  -4.0516\n",
      "\n",
      "Columns 8 to 15 \n",
      "   -5.9114  -5.1948 -11.3433 -11.0021 -11.7250 -10.5584 -10.7439 -11.4594\n",
      "\n",
      "Columns 16 to 23 \n",
      "  -11.4089 -11.2947 -11.5765 -10.3599  -5.9444  -6.5657 -12.1545  -6.3474\n",
      "\n",
      "Columns 24 to 31 \n",
      "  -12.3250 -12.8841 -11.4574 -11.3507 -12.2885 -11.7068 -12.2962 -12.1997\n",
      "\n",
      "Columns 32 to 39 \n",
      "  -11.4052 -12.2497 -12.2313 -11.1457 -12.4977 -11.2904 -13.2086 -12.2042\n",
      "\n",
      "Columns 40 to 47 \n",
      "  -12.4756 -11.4500 -11.3456 -11.9498 -13.3552 -11.5114 -12.4673 -12.3596\n",
      "\n",
      "Columns 48 to 55 \n",
      "  -10.6597 -12.0614 -10.7478 -10.1413  -9.0070  -3.5699  -7.2681  -5.0385\n",
      "\n",
      "Columns 56 to 63 \n",
      "   -1.8331  -4.0609  -5.1368  -5.8949  -8.3531  -5.0306  -9.1745  -6.4868\n",
      "\n",
      "Columns 64 to 71 \n",
      "   -3.6384  -3.0826  -2.0113  -5.8587  -4.6970  -6.5899  -1.0319  -3.4709\n",
      "\n",
      "Columns 72 to 79 \n",
      "   -4.9900  -7.0807  -4.6187  -5.8892  -6.9336  -5.5227 -11.7154 -13.0600\n",
      "\n",
      "Columns 80 to 84 \n",
      "  -11.6646 -12.4337 -12.3816 -11.2071 -12.9071\n",
      "\n",
      "(2 ,.,.) = \n",
      "\n",
      "Columns 0 to 7 \n",
      "   -3.5443  -1.0347  -6.1617  -5.9945  -8.7254 -11.3206  -6.4187  -3.4414\n",
      "\n",
      "Columns 8 to 15 \n",
      "   -4.9713  -4.1580 -14.0243 -12.2827 -12.1167 -12.7218 -12.6964 -12.0618\n",
      "\n",
      "Columns 16 to 23 \n",
      "  -12.6790 -12.8566 -13.1504 -13.0366  -5.7430  -5.6823  -9.4414  -6.0458\n",
      "\n",
      "Columns 24 to 31 \n",
      "  -12.4607 -12.3702 -11.6000 -11.5271 -10.9307 -11.9680  -9.9306 -13.5221\n",
      "\n",
      "Columns 32 to 39 \n",
      "  -12.1629 -11.9210 -11.8109 -13.1695 -11.2219 -11.9740 -12.6568 -12.3912\n",
      "\n",
      "Columns 40 to 47 \n",
      "  -13.6314 -13.7145 -10.8798 -12.0723 -12.6449 -12.6192 -13.2768 -12.6478\n",
      "\n",
      "Columns 48 to 55 \n",
      "  -11.2424 -12.2047  -9.6119  -8.3444  -9.2486  -7.3769  -8.7689  -4.6179\n",
      "\n",
      "Columns 56 to 63 \n",
      "   -8.7249  -4.5529  -6.3525  -8.1530  -5.3836  -4.9663  -9.4073  -7.0425\n",
      "\n",
      "Columns 64 to 71 \n",
      "   -5.8684  -4.9953  -6.7543  -5.8086  -3.0388  -9.1043  -9.2618  -2.8456\n",
      "\n",
      "Columns 72 to 79 \n",
      "   -0.9580  -5.6420  -8.4429  -6.9025 -12.6850  -7.3552  -8.8679 -12.5228\n",
      "\n",
      "Columns 80 to 84 \n",
      "  -11.9544 -12.5193 -12.4567 -12.2504 -12.5890\n",
      "\n",
      "(3 ,.,.) = \n",
      "\n",
      "Columns 0 to 7 \n",
      "   -3.2093  -0.6366  -6.1699  -5.1688  -8.3390  -9.9330  -6.8368  -3.3703\n",
      "\n",
      "Columns 8 to 15 \n",
      "   -3.4526  -4.9181 -13.2642 -13.1785 -13.1175 -12.3327 -12.6795 -12.0743\n",
      "\n",
      "Columns 16 to 23 \n",
      "  -13.1560 -13.2169 -13.1301 -12.9573  -6.2410  -6.0765  -7.4876  -6.0872\n",
      "\n",
      "Columns 24 to 31 \n",
      "   -9.6826 -11.5981 -11.3191 -11.2358  -9.5967 -11.0137 -10.7691 -10.8622\n",
      "\n",
      "Columns 32 to 39 \n",
      "   -9.6816 -12.1685 -12.3823 -10.0703 -11.0692 -11.0864 -10.5553 -12.2322\n",
      "\n",
      "Columns 40 to 47 \n",
      "  -12.7412 -11.5183  -9.6010 -11.6049 -11.1657 -12.7147 -12.6736 -12.0436\n",
      "\n",
      "Columns 48 to 55 \n",
      "  -11.3078 -11.8512  -9.1980  -9.2862  -7.7894  -2.9857  -8.6971  -8.6237\n",
      "\n",
      "Columns 56 to 63 \n",
      "   -9.8754  -3.5420  -7.7289  -9.1720  -4.5045  -2.0386 -12.9805 -11.0188\n",
      "\n",
      "Columns 64 to 71 \n",
      "   -3.7417  -6.5813  -6.4952  -4.4659  -9.2056  -9.5276  -3.3230  -3.9667\n",
      "\n",
      "Columns 72 to 79 \n",
      "   -5.8533  -4.7280 -10.5722  -7.1554 -11.7522  -4.4013 -11.3564 -12.7866\n",
      "\n",
      "Columns 80 to 84 \n",
      "  -11.4547 -12.3710 -12.1714 -12.5227 -12.6076\n",
      "[torch.cuda.FloatTensor of size 4x1x85 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = 'test'\n",
    "idxs = T(np.array([chars_to_indexes[c] for c in inp]))\n",
    "p = m(*VV(idxs))\n",
    "\n",
    "print(p)\n",
    "\n",
    "# Get the next n letters given an input block\n",
    "def get_next(inp):\n",
    "    idxs = T(np.array([chars_to_indexes[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]\n",
    "\n",
    "def get_next_n(inp, n):\n",
    "    for i in range(0, n):\n",
    "        inp += get_next(inp[-3:])\n",
    "    \n",
    "    return inp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.74882, -0.261  ,  0.38523],\n",
      "       [-2.34712,  0.14807, -0.2576 ]]), array([[-0.31029,  1.4766 , -0.789  ],\n",
      "       [-0.09989, -0.12712, -0.27415]]), array([[ 0.12574,  1.18033,  1.25824],\n",
      "       [-0.38231, -0.33968, -1.48819]]), array([[ 0.56883,  0.65055,  1.24903],\n",
      "       [ 1.42539,  1.38552, -1.32857]])]\n",
      "[[[ 0.74882 -0.261    0.38523]\n",
      "  [-2.34712  0.14807 -0.2576 ]]\n",
      "\n",
      " [[-0.31029  1.4766  -0.789  ]\n",
      "  [-0.09989 -0.12712 -0.27415]]\n",
      "\n",
      " [[ 0.12574  1.18033  1.25824]\n",
      "  [-0.38231 -0.33968 -1.48819]]\n",
      "\n",
      " [[ 0.56883  0.65055  1.24903]\n",
      "  [ 1.42539  1.38552 -1.32857]]]\n",
      "(4, 2, 3)\n",
      "[[[ 0.74882 -0.261    0.38523]\n",
      "  [-0.31029  1.4766  -0.789  ]\n",
      "  [ 0.12574  1.18033  1.25824]\n",
      "  [ 0.56883  0.65055  1.24903]]\n",
      "\n",
      " [[-2.34712  0.14807 -0.2576 ]\n",
      "  [-0.09989 -0.12712 -0.27415]\n",
      "  [-0.38231 -0.33968 -1.48819]\n",
      "  [ 1.42539  1.38552 -1.32857]]]\n",
      "(2, 4, 3)\n",
      "[[[ 0.74882 -0.31029  0.12574  0.56883]\n",
      "  [-0.261    1.4766   1.18033  0.65055]\n",
      "  [ 0.38523 -0.789    1.25824  1.24903]]\n",
      "\n",
      " [[-2.34712 -0.09989 -0.38231  1.42539]\n",
      "  [ 0.14807 -0.12712 -0.33968  1.38552]\n",
      "  [-0.2576  -0.27415 -1.48819 -1.32857]]]\n",
      "(2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "arr = [np.random.randn(2, 3) for _ in range(4)]\n",
    "print(arr)\n",
    "\n",
    "print(np.stack(arr, axis=0))\n",
    "print(np.stack(arr, axis=0).shape)\n",
    "\n",
    "print(np.stack(arr, axis=1))\n",
    "print(np.stack(arr, axis=1).shape)\n",
    "\n",
    "print(np.stack(arr, axis=2))\n",
    "print(np.stack(arr, axis=2).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
